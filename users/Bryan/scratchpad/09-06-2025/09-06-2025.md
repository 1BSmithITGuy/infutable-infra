# 09-06-2025
 

--------## ***previous 2 days - Talos deployment:***--------
--------------------------------------------------------------------


## (09-04)

talos notes:

### **generate config:**

Make the config directory in the repo and run:
```bash
talosctl gen config "us103-talos01" "https://bsus103tal-k8m01.k8s.infutable.com:6443" --output-dir config --with-examples=false --with-docs=false --kubernetes-version 1.34.0
```

create yaml config files for control plane and 2 workers and merge those with the talos config:

```bash
talosctl machineconfig patch config/controlplane.yaml --patch @config/patches/control-plane.yaml --output config/controlplane-final.yaml

talosctl machineconfig patch config/worker.yaml --patch @config/patches/worker-01.yaml --output config/worker-01-final.yaml

talosctl machineconfig patch config/worker.yaml --patch @config/patches/worker-02.yaml --output config/worker-02-final.yaml
```

## (09-05)

---

#  Create VMs

### ISO download:  factory.talos.dev
- bare metal iso
- get the xen extension, and update the config:

```yaml
machine:
  systemExtensions:
    officialExtensions:
      - siderolabs/xen-guest-agent
```

## VM specs:
```
Control plane: 
- 4 GB RAM, 2 vCPU, 30GB disk
- bsus103tal-k8m01.k8s.infutable.com IP:  10.0.2.2/27

Workers (2):
- 4 GB RAM, 2 vCPU, 50GB disk
- bsus103tal-k8w01.k8s.infutable.com → 10.0.2.3/27
- bsus103tal-k8w02.k8s.infutable.com → 10.0.2.4/27
```

**Steps:**
1. Use "other install media" template

2. Boot from ISO on a VLAN with DHCP

3. Apply config to control plane node:
```bash
talosctl apply-config --insecure --nodes <IP_of_CP_node> --file controlplane-final.yaml
```
- switch to other network once IP changes

4. Copy the talosconfig file that was generated earlier to the file:  ***~/.talos/config***

5. Run the below command:
```bash
talosctl bootstrap --nodes 10.0.2.2 --endpoints 10.0.2.2
```

6.  
```bash
talosctl --nodes 10.0.2.2 --endpoints 10.0.2.2 config endpoint 10.0.2.2
```

7. 
```bash
talosctl kubeconfig --nodes 10.0.2.2 --force --merge
```

8. Install Cilium CNI (also see docs/cilium-talos-readme.md for other info, but this is what worked).
```bash
helm install cilium cilium/cilium \
  --namespace kube-system \
  --set ipam.mode=kubernetes \
  --set kubeProxyReplacement=true \
  --set securityContext.capabilities.ciliumAgent="{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}" \
  --set securityContext.capabilities.cleanCiliumState="{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}" \
  --set cgroup.autoMount.enabled=false \
  --set cgroup.hostRoot=/sys/fs/cgroup \
  --set k8sServiceHost=localhost \
  --set k8sServicePort=7445
  ```


9. Boot from ISO in DHCP VLAN and apply config to workers:
**NOTE:**  Like the CP node, switch to the correct VLAN once IP changes in console.
```bash
# worker 1:
talosctl apply-config --insecure --nodes <WORKER1_IP> --file worker-01-final.yaml

# worker 2:
talosctl apply-config --insecure --nodes <WORKER2_IP> --file worker-02-final.yaml
```






Replicas?  helm upgrade cilium cilium/cilium -n kube-system --set operator.replicas=2


***NEXT:***
    
```
- uncordon nodes
- replicate nodes as is.
- What should I deploy through argo or rancher below?

- kubeproxy replacement - true    ?  With cilium.  Do I want this?  do I need this?
- metallb, should I do rancher first, deploy through there?  
- setup pod backups on kubeadm and k3s cluster (valero?  veeam? maybe both, veeam for talos?)
        - maybe setup NFS, static IPs for truenas, and backup repo for XO backups with nightly talos backups?
- migrate apps
- setup XO backups/repo
- merge with main in repo
```

--------## ***09-06-2025 - Talos deployment:***--------
--------------------------------------------------------------------

**Cilium kube proxy replacement**

```bash
helm upgrade cilium cilium/cilium -n kube-system --set kubeProxyReplacement=true
```
***snapshot***

**troubleshooting**

Fixed Cilium, updated above process:
```bash
helm uninstall cilium -n kube-system

helm install cilium cilium/cilium \
  --namespace kube-system \
  --set ipam.mode=kubernetes \
  --set kubeProxyReplacement=true \
  --set securityContext.capabilities.ciliumAgent="{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}" \
  --set securityContext.capabilities.cleanCiliumState="{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}" \
  --set cgroup.autoMount.enabled=false \
  --set cgroup.hostRoot=/sys/fs/cgroup \
  --set k8sServiceHost=localhost \
  --set k8sServicePort=7445


# Since Cilium is running with `kubeProxyReplacement=true`, kube-proxy is redundant:

kubectl patch daemonset kube-proxy -n kube-system -p '{"spec":{"template":{"spec":{"nodeSelector":{"non-existing-node": "true"}}}}}'

```

***NEXT:***

- What should I deploy through argo or rancher below?

- kubeproxy replacement - true    ?  With cilium.  Do I want this?  do I need this?
- metallb, should I do rancher first, deploy through there?  
- setup pod backups on kubeadm and k3s cluster (valero?  veeam? maybe both, veeam for talos?)
- maybe setup NFS, static IPs for truenas, and backup repo for XO backups with nightly talos backups?
- migrate apps
- setup XO backups/repo
- merge with main in repo

- replicate nodes 
- finish documentaation above (fill out steps)

